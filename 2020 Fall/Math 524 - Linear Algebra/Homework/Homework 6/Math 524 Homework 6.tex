\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{color}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}
\tikzset{main node/.style={circle,fill=gray!20,draw,minimum size=.5cm,inner sep=0pt},}

\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codewhite}{rgb}{1,1,1}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblack}{rgb}{0,0,0}
\definecolor{codeorange}{rgb}{0.8,0.4,0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{codewhite},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codegreen},
    numberstyle=\color{codegray},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily ,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}


\setlength{\hoffset}{-1in}
\addtolength{\textwidth}{1.5in}
\setlength{\voffset}{-1in}
\addtolength{\textheight}{1.5in}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\BigO}[1]{\ensuremath\mathcal{O}\left(#1\right)}
\newcommand{\il}[1]{\lstinline!#1!}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\parens}[1]{\left(#1\right)}
\newcommand{\bracks}[1]{\left\{#1\right\}}
\newcommand{\sqbracks}[1]{\left[#1\right]}
\newcommand{\vep}{\varepsilon}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\LT}{\mathcal{L}}
\newcommand{\poly}{\mathcal{P}}
\newcommand{\range}[1]{\text{range }#1}
\renewcommand{\null}[1]{\text{null }#1}
\newcommand{\distrib}[2]{\text{#1}\left(#2\right)}
\newcommand{\dd}[1]{\frac{d}{d#1}}
\newcommand{\abracks}[1]{\left< #1\right>}

\begin{document}
	\begin{center}
		\textbf{Fall 2020, Math 524:\ Homework 6} \\
		\textbf{Due:\ Monday, November 2nd, 2020} \\
		\textbf{Joseph Diaz: 819947915}
	\end{center}
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}

\subsection*{6A}
\be
    \item[1.] Show that the function $\varphi: \R^2\times\R^2 \to \R$ such 
    that
    $$\varphi\parens{\vec{x}, \vec{y}} = \abs{x_1y_1} + \abs{x_2y_2}$$
    for $\vec{x}, \vec{y} \in \R^2$ is not an inner product on $\R^2$.
    \begin{proof}
    $\varphi$ is not an inner product on $\R^3$ because it doesn't satisfy 
    additivity in the first slot. Let $\vec{x}, \vec{y},\vec{z} \in \R^2$, 
    then 
    \begin{align*}
    \varphi\parens{\vec{x} + \vec{y}, \vec{z}} &= \abs{(x_1 + y_1)z_1} 
    + \abs{(x_2 + y_2)z_2} = \abs{x_1z_1 + y_1z_1} + \abs{x_2z_2 + 
    y_2z_2}\\
    &\leq\abs{x_1z_1} + \abs{y_1z_1} + \abs{x_2z_2} + \abs{y_2z_2} =
    \abs{x_1z_1} + \abs{x_2z_2} + \abs{y_1z_1} + \abs{y_2z_2} \\
    &= \varphi\parens{\vec{x}, \vec{z}} + \varphi\parens{\vec{y}, \vec{z}}
    \end{align*}
    So we have that it isn't necessarily the case that 
    $\varphi\parens{\vec{x} + \vec{y}, \vec{z}}$ equals 
    $\varphi\parens{\vec{x}, \vec{z}} + \varphi\parens{\vec{y}, \vec{z}}$.
    Therefore, $\varphi$ is not an inner product.
    \end{proof}

    \item[2.] Show that the function $\varphi: \R^3\times\R^3 \to \R$ such 
    that
    $$\varphi\parens{\vec{x}, \vec{y}} = x_1y_1 + x_3y_3$$
    for $\vec{x}, \vec{y} \in \R^3$ is not an inner product on $\R^2$.
    \begin{proof}
    $\varphi$ is not an inner product on $\R^3$ because it doesn't satisfy 
    definiteness. Let $\vec{x} \in\R^3$ such that 
    $\vec{x} = (0,\ 1,\ 0)$, then
    $$\varphi\parens{\vec{x},\vec{x}} = 0\cdot0 + 0\cdot0 = 0$$
    but $\vec{x} \neq \vec{0}$. Therefore, $\varphi$ is not an inner 
    product.
    \end{proof}

    \item[4.] Suppose $V$ is a real inner product space.
    \be[(a)] 
        \item Show that $\abracks{u+v, u-v} = \norm{u}^2 - \norm{v}^2$ for 
        every $u,v \in V$.
        \begin{proof}
        Let $u,v \in V$, then by additivity in the first slot, we have that 
        $$\abracks{u+v, u-v} = \abracks{u, u-v} + \abracks{v,u-v}$$
        $V$ is a real inner product space, this means that by symmetry
        $$\abracks{u, u-v} + \abracks{v,u-v}= 
        \abracks{u-v, u} + \abracks{u-v,v}$$
        and again by additivity in the first slot
        $$\abracks{u-v, u} + \abracks{u-v,v} = 
        \abracks{u, u} - \abracks{v,u}+\abracks{u, v} - \abracks{v,v}$$
        now, by symmetry and the definion of a norm, we have 
        $$\abracks{u, u} - \abracks{v,u}+\abracks{u, v} - \abracks{v,v}=
        \norm{u}^2 - \abracks{u,v}+\abracks{u,v} -\norm{v}^2$$
        Finally, this yields:
        $$\abracks{u+v, u-v} = \norm{u}^2 - \norm{v}^2$$

        \end{proof}

        \item Show that if $u,v \in V$ have the same norm, then 
        $u+v \perp u-v$.
        \begin{proof}
        Let $u,v\in V$ such that $\norm{u} = \norm{v}$, which implies that 
        $\norm{u}^2 = \norm{v}^2$ and 
        $$\norm{u}^2 - \norm{v}^2 = 0$$
        By part~(a), this means that 
        $$\norm{u}^2 - \norm{v}^2 = \abracks{u+v, u-v} = 0$$
        which is equivalent to $u+v \perp u-v$.
        \end{proof}

        \item Use part~(b) to show that the diagonals of a rhombus are 
        perpendicular to each other.
        \begin{proof}
        If the vectors $u$ and $v$ from part~(b) represent adjacent sides 
        of a parallelogram, then that 
        parallelogram is also a rhombus because the lengths of $u$ and $v$ 
        are equal. The vector that would bissect the angle made between $u$
        and $v$ would be $u + v$ by the parallelogram rule of vector 
        addition and would represent one of the diagonals of 
        the rhombus. The other diagonal of the rhombus would the the vector
        $u - v$ by the tip-to-tail rule of vector addition. From part~(b),
        we know that 
        $$\forall u,v \in V,\ \norm{u} = \norm{v} \iff \abracks{u+v,u-v} = 
        0$$ 
        This means that $u+v\perp u-v$; and since those vectors represent 
        the diagonals of an arbitrary rhombus, they are always orthogonal.
        \end{proof}
    \ee
    
    \item[5.] 
    Suppose that $T \in \LT\parens{V}$ such that $\norm{Tv} \leq 
    \norm{v}$ for every $v \in V$. Prove that $T - \sqrt{2}I$ is 
    invertible.
    \begin{proof}
    Suppose that $V$ is finite dimensional, we'll show that 
    $T-\sqrt{2}I$ is invertible by showing that it is injective. 
    Let $u \in \text{null}\parens{T-\sqrt{2}I}$, then
    \begin{gather*}
    \parens{T-\sqrt{2}I}u = Tu - \sqrt{2}u = 0 \\
    \implies Tu = \sqrt{2}u \\
    \implies \norm{Tu} = \sqrt{2}\norm{u}
    \end{gather*}
    By the definition of $T$, this means that $\sqrt{2}\norm{u}\leq 
    \norm{u}$. The only case in which this inequality holds is when 
    $\norm{u} = 0$, which implies that $u=0$. This means that 
    $\text{null}\parens{T-\sqrt{2}I} = \bracks{0}$; so $T-\sqrt{2}I$ is 
    injective, and, consequently, invertible.
    \end{proof}

\ee

\subsection*{6B}
\be
    \item[1.]
    \be[(a)] 
        \item Suppose $\theta \in \R$. Show that 
        $(\cos \theta, \sin \theta),\ (-\sin \theta, \cos \theta)$ and 
        $(\cos \theta, \sin \theta),\ (\sin \theta, -\cos \theta)$ are 
        othonormal bases of $\R^2$.
        \begin{proof}
        Each of the vectors in both bases is a unit vector, because each 
        has a norm of 
        $$\sqrt{\parens{\cos\theta}^2 + \parens{\sin \theta}^2} = 1$$
        by the Pythagorean Identity, and the vectors in each basis are 
        orthogonal to each other because
        \begin{gather*}
        \abracks{(\cos \theta, \sin \theta),\ (-\sin \theta, \cos \theta)} 
        = -\cos\theta\sin\theta + \sin\theta\cos\theta = 0\\
        \abracks{(\cos \theta, \sin \theta),\ (\sin \theta, -\cos \theta)} 
        = \cos\theta\sin\theta - \sin\theta\cos\theta = 0
        \end{gather*}
        So each set forms an orthonormal basis of $\R^2$.
        \end{proof}

        \item Show that each orthonormal basis of $\R^2$ is of the form 
        given by one of the two possibilities of part ~(a).
        \begin{proof}
        Any pair of unit vectors $u,v \in \R^2$ can be written as 
        $u = \parens{\cos\theta, \sin\theta}$ and 
        $v = \parens{\cos\omega, \sin\omega}$, for some angles 
        $\theta,\omega \in \R$. For these vectors to form an orthonormal
        basis of $\R^2$, we must find $\theta$ and $\omega$ such that
        $$\abracks{u,v} = \cos\theta\cos\omega + \sin\theta\sin\omega = 0$$
        using trigonometric identities this is equivalent to 
        $\cos\parens{\theta - \omega} = 0$. Fixing $\theta$ and letting
        $\omega = \theta + \pi/2$ solves this equation because 
        $\cos(-\pi/2) = 0$, then
        \begin{align*}
        \parens{\cos\omega,\sin\omega} &= \parens{\cos\parens{\theta+
        \frac{\pi}{2}}, \sin\parens{\theta+\frac{\pi}{2}}} \\
        &= \parens{\cos\theta\cos(\pi/2) - \sin\theta\sin(\pi/2),
        \sin\theta\cos(\pi/2) + \sin(\pi/2)\cos\theta} \\
        &= \parens{-\sin\theta, \cos\theta}
        \end{align*}
        This gives us $(\cos \theta, \sin \theta),\ 
        (-\sin \theta, \cos \theta)$, which is the first set of basis 
        vectors from part~(a). Fixing $\omega$ and letting $\theta = \omega 
        - \pi/2$ also solves the equation, and 
        \begin{align*}
        \parens{\cos\theta,\sin\theta} &= \parens{\cos\parens{\omega-
        \frac{\pi}{2}}, \sin\parens{\omega-\frac{\pi}{2}}} \\
        &= \parens{\cos\omega\cos(\pi/2) + \sin\omega\sin(\pi/2),
        \sin\omega\cos(\pi/2) - \sin(\pi/2)\cos\omega} \\
        &= \parens{\sin\omega, -\cos\omega}
        \end{align*}
        This gives us $(\cos \omega, \sin \omega),\ 
        (\sin \omega, -\cos \omega)$, which is equivalent to the second 
        basis from part~(a), up to a renaming of the angle variable.
        Therefore, any orthonormal basis of $\R^2$ is equivalent to one of 
        the basis of part~(a).        
        \end{proof}
    \ee

    \item[3.] Suppose $T \in \LT\parens{\R^3}$ has an upper-triangular
    matrix with respect to the basis $(1, 0, 0),\ (1, 1, 1),\ (1, 1, 2)$. 
    Find an orthonormal basis of $\R^3$ (use the usual inner product on 
    $\R^3$) with respect to which $T$ has an upper-triangular matrix.
    \begin{proof}[Answer]
    To find this basis, we'll use the Gram–Schmidt Procedure. Label the 
    three given basis vectors as $v_1, v_2, v_3$ respectively, and let 
    $e_1 = v_1$, since $v_1$ is already a unit vector. Then we have that 
    \begin{align*}
    e_2 &= \frac{v_2 - \abracks{v_2, e_1}e_1}{\norm{v_2 - 
    \abracks{v_2, e_1}e_1}} = \frac{(1,1,1) - (1,0,0)}{\norm{(1,1,1) - 
    (1,0,0)}} \\
    &= \frac{1}{\sqrt{2}}\parens{0,1,1} = \parens{0, \frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}}}
    \end{align*}
    and 
    \begin{align*}
    e_3 &= \frac{v_3 - \abracks{v_3, e_1}e_1 - \abracks{v_3, e_2}e_2}{
    \norm{v_2 - \abracks{v_3, e_1}e_1- \abracks{v_3, e_2}e_2}} = 
    \frac{(1,1,2) - (1,0,0) - \parens{0, 3/2, 3/2}}
    {\norm{(1,1,2) - (1,0,0) - \parens{0, 3/2, 3/2}}} \\
    &= \sqrt{2}\parens{0,-\frac{1}{2},\frac{1}{2}} = 
    \parens{0, -\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}}
    \end{align*}
    By 6.37 in the text, this means that $T$ with respect to the orthonormal
    basis
    $$e_1=(1,0,0),\ e_2=\parens{0, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}},
    \ e_3=\parens{0, -\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}}$$
    has an upper-triangular matrix.
    \end{proof}
    
    \item[9.] What happens if the Gram–Schmidt Procedure is applied to a 
    list of vectors that is not linearly independent?
    \begin{proof}[Answer]
    Suppose $v_1,\ \cdots,\ v_n$ is a linearly dependent list of vectors
    in $V$, then $j,\ 1\leq j< n$, be the smallest number such that a 
    subset of the 
    list of length $j$ is linearly independent in $V$; we denote the span 
    of this list as $W$ and there exists $v_k \in W$. Applying the 
    Gram–Schmidt Procedure on these $j$ vectors produces an orthonormal
    basis $e_1,\ \cdots,\ e_j$ that also spans $W$. By 6.30 in text, $v_k$
    can be written as
    $$v_k = \abracks{v_k, e_1}e_1 + \cdots \abracks{v_k, e_j}e_j$$
    However; if we continued the Gram–Schmidt Procedure on the rest of 
    the vectors in the orginal list, then the right hand side of the 
    equation is what we would need to subtract from $v_k$, yielding
    the zero vector. Any linearly dependent vectors like this must be 
    discarded and we find that $W = \text{span}\bracks{v_1,\ \cdots,\ v_n}$.
    \end{proof}

        
\ee

\subsection*{6C}
\be
    \item[5.] Suppose $V$ is finite-dimensional and $U$ is a subspace of 
    $V$. Show that $P_{U^\perp} = I - P_U$, where $I$ is the identity 
    operator on $V$.
    \begin{proof}
    Since $V$ is finite dimensional, we have that $V = U \oplus U^{\perp}$ 
    and for all $v \in V,\ v = u + w$ for $u\in U$ and $w \in U^\perp$. 
    By the definition of orthogonal projections, we also have that 
    $P_U(v) = u$ and $P_{U^\perp}(v) = w$. Therefore,
    $$P_{U^\perp}(v) = w = w + u - u = v - P_U(v) = (I - P_U)(v)$$
    As $v$ is represents any vector in $V$, this implies that 
    $P_{U^\perp} = I - P_U$, as desired.
    \end{proof}
    
    \item[11.] In $\R^4$, let 
    $$U = \text{span}\parens{(1,1,0,0),\ (1,1,1,2)}$$
    Find $u \in U$ such that $\norm{u - (1,2,3,4)}$ is as small as 
    possible. 
    \begin{proof}[Answer]
    First, we start by find an orthonormal basis of $U$ using the 
    Gram–Schmidt Procedure. Let $e_1 = \frac{1}{\sqrt{2}}(1,1,0,0)=
    \parens{\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}},0,0}$ then 
    $$e_2 = \frac{\parens{1,1,1,2}- (2/\sqrt{2})
    \parens{\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}},0,0}}
    {\norm{\parens{1,1,1,2}- (2/\sqrt{2})
    \parens{\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}},0,0}}} = 
    \parens{0,0,\frac{1}{\sqrt{5}}, \frac{2}{\sqrt{5}}}$$
    so $\parens{\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}},0,0}, 
    \parens{0,0,\frac{1}{\sqrt{5}}, \frac{2}{\sqrt{5}}}$ form an 
    orthonormal basis of $U$. So the solution to our minimization problem
    is $P_U((1,2,3,4))$; which, by 6.55 in the text, is
    \begin{align*}
    P_U((1,2,3,4)) &= \abracks{(1,2,3,4), \parens{\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}},0,0}}\parens{\frac{1}{\sqrt{2}},
    \frac{1}{\sqrt{2}},0,0} \\
    &\qquad + \abracks{(1,2,3,4), \parens{0,0,\frac{1}{\sqrt{5}},
    \frac{2}{\sqrt{5}}}}\parens{0,0,\frac{1}{\sqrt{5}},
    \frac{2}{\sqrt{5}}} \\
    &= \parens{\frac{3}{2}, \frac{3}{2},0,0} + \parens{0,0,
    \frac{11}{5},\frac{22}{5}} \\
    &= \parens{\frac{3}{2}, \frac{3}{2}, \frac{11}{5},\frac{22}{5}}
    \end{align*}
    so $u = \parens{\frac{3}{2}, \frac{3}{2}, \frac{11}{5},\frac{22}{5}}$.
    \end{proof}
        
\ee
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
	
\end{document}
 