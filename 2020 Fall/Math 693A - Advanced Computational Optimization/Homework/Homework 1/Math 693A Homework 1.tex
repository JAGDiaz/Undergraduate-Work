\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{color}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}
\tikzset{main node/.style={circle,fill=gray!20,draw,minimum size=.5cm,inner sep=0pt},}

% In line code stuff%
\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codewhite}{rgb}{1,1,1}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblack}{rgb}{0,0,0}
\definecolor{codeorange}{rgb}{0.8,0.4,0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{codewhite},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codegreen},
    numberstyle=\color{codegray},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily ,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}
\setlength{\hoffset}{-1in}
\addtolength{\textwidth}{1.5in}
\setlength{\voffset}{-1in}
\addtolength{\textheight}{1.5in}

% Custom commands%
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\BigO}[1]{\ensuremath\mathcal{O}\left(#1\right)}
\newcommand{\il}[1]{\lstinline!#1!}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\parens}[1]{\left(#1\right)}
\newcommand{\bracks}[1]{\left\{#1\right\}}
\newcommand{\sqbracks}[1]{\left[#1\right]}
\newcommand{\vep}{\varepsilon}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\distrib}[2]{\text{#1}\left(#2\right)}
\newcommand{\dd}[1]{\frac{d}{d#1}}
\newcommand{\abracks}[1]{\left< #1\right>}

\begin{document}
	\begin{center}
		\textbf{Fall 2020, Math 693A:\ Homework 1} \\
		\textbf{Due:\ Friday, September 18th, 2020} \\
		\textbf{Joseph Diaz: 819947915}
	\end{center}
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
\be 
\item Program the steepest descent and Newton algorithms using the 
    backtracking line search. Use them to minimize the Rosenbrock
    function
    $$f\parens{\vec{x}} = 100\parens{x_2 - x_1^2}^2 + \parens{1-x_1}
    ^2$$
    Set the initial step length $\alpha_0 = 1$ and report the step 
    length 
    used by each method at each iteration. First try the initial     
    point 
    $\vec{x}_0^T = \sqbracks{1.2, 1.2}$ and then the more difficult 
    point
    $\vec{x}_0^T = \sqbracks{-1.2, 1}$.\\\\
    Suggested values: $\bar{\alpha} =1,\ \rho=\frac{1}{2},\ c = 
    10^{-4}$.
    \\\\
    Stop when: $\abs{f\parens{\vec{x}_k}} < 10^{-8}$ or $\norm{\nabla 
    f\parens{\vec{x}_k}} < 10^{-8}$\\\\
    You should hand in (i) your code (ii) the first 6 and last 6 
    values of $\vec{x}_k$ obtained from your program for steepest 
    descent and Newton algorithms and (iii) determine the minimizer 
    of the Rosenbrock function $x^*$.
    \be[(i)]
    \item Code is attached in a file called ``\il{H1P1.py}''.
    
    \item The follow table has the desired information:
    \begin{table}[h!]
    \begin{tabular}{|c||p{3.74cm}|p{3.74cm}|p{3.74cm}|p{3.74cm}|}
    \hline
    & \multicolumn{2}{|c|}{Steepest Descent} & 
    \multicolumn{2}{|c|}{Newton's} \\
    \hline\hline
    $\vec{x}_0$ & $\sqbracks{1.2, 1.2}$ & $\sqbracks{-1.2, 1}$ & 
    $\sqbracks{1.2, 1.2}$ & $\sqbracks{-1.2, 1}$ \\
    \hline
    First six & 
    [1.08455638, 1.24793507]
    [1.11290849, 1.23479275]
    [1.11109258, 1.2355119 ]
    [1.11145306, 1.23518255]
    [1.11096726, 1.23523167]
    [1.1113276, 1.23490216] & 
    [-0.9685380, 1.09447425]
    [-1.0779672, 1.0340568 ]
    [-1.0205784, 1.05881116]
    [-1.0257012, 1.0529127 ]
    [-1.0178969, 1.05255455]
    [-1.0228084, 1.04647895]&
    [1.19687506, 1.43269212]
    [1.13851273, 1.29338155]
    [1.06316325, 1.12465211]
    [1.03143189, 1.06284471]
    [1.00597933, 1.01134676]
    [1.00124807, 1.00247532] &
    [-1.1682636, 0.92336843]
    [-1.1590085, 1.35167617]
    [-0.9684961, 0.90883947]
    [-0.8223798, 0.65830036]
    [-0.6539566, 0.40405665]
    [-0.4479908, 0.14811196] \\
    \hline
    Last six & 
    [1.00009476, 1.00017282]
    [1.00008103, 1.00017949]
    [1.00009462, 1.00017253]
    [1.00008089, 1.00017921]
    [1.00009448, 1.00017225]
    [1.00008762, 1.00017559]& 
    [0.9999188, 0.99982021]
    [0.99990522, 0.99982716]
    [0.99991894, 0.99982049]
    [0.99990536, 0.99982744]
    [0.99991909, 0.99982077]
    [0.99991229, 0.99982424]&
    [1.19687506, 1.43269212]
    [1.13851273, 1.29338155]
    [1.06316325, 1.12465211]
    [1.03143189, 1.06284471]
    [1.00597933, 1.01134676]
    [1.00124807, 1.00247532] & 
    [0.91125263, 0.82107844]
    [0.94123081, 0.88501226]
    [0.98895058, 0.9757461 ]
    [0.99668226, 0.99331584]
    [0.99999882, 0.99998664]
    [1.,         1.        ]\\
    \hline
    \end{tabular}
    \end{table}
    The first and last six solutions of the Newton's Method with 
    $\vec{x}_0 = \sqbracks{1.2,\ 1.2}$ look the same, because they 
    are the same. That method and starting point only took 6 
    iteration.
    
    \item The minimizer of the Rosenbrock Function is $\vec{x}^* 
    = \sqbracks{1,\ 1}$.
    
    \ee

\item Using the $\vec{x}_k$ values you obtained in Problem 1 
    determine
    \be[(i)]
    \item the rate of convergence of the steepest descent algorithm.
   
    \item the rate of convergence of the Newton algorithms.
    
    \ee
    You should show work to obtain full credit (show your 
    calculations or your code if you computed it numerically).
    \begin{proof}[Answer]
    The results below represent the value $p$ in the inequality 
    $$\frac{\norm{\vec{x}_{k+1} - \vec{x}^*}}
    {\norm{\vec{x}_{k} - \vec{x}^*}^p} \leq C, C \in \R^+$$
    and $p$ is approximated by taking the natural log of the 
    inequality and using linear regression on the logs of the norms
    for the different values of $\vec{x}_k$.
    The results below are copied directly from the output 
    of \il{H1P1.py} program.  
 \begin{lstlisting}[language=python]
Steepest Descent Method
Rate of convergence, x0 = (1.2, 1.2): 1.000015
Rate of convergence, x0 = (1.2, -1): 0.999845

Newtons Method: 
Rate of convergence, x0 = (1.2, 1.2): 2.143219
Rate of convergence, x0 = (1.2, -1): 1.724124
\end{lstlisting}
    Represented visually:
    \begin{figure}[h!]
  	\includegraphics[width=\linewidth]{Log10ofFunc.png}
  	\centering
	\end{figure}
    \end{proof}
\item Determine if the following function of two variables is convex.     
    Create a contour plot of the function using a programming
    language of your choice:
    $$f\parens{x,y} = 5-5x-2y + 2x^2 +5xy + 6y^2$$
    \begin{proof}[Answer]
    To determine if $f$ is convex we'll use the property:\\
    $f: \R^n \to \R$ is a convex function if and only if it's 
    Hessian matrix is positive semi-definite.\\
    So with this we have:
    $$\nabla f(x,y) = \sqbracks{\begin{matrix}
    -5 +4x +5y \\
    -2 + 5x +12y 
    \end{matrix}}\implies \nabla^2f(x,y) = \sqbracks{\begin{matrix}
    4 & 5 \\
    5 & 12 
    \end{matrix}}$$
    From linear algebra, we know that a matrix is positive 
    semi-definite if and only if it's eigenvalues are non-negative, 
    so:
    $$\det\parens{\nabla^2 f - \lambda I} = 0 \implies 
    \abs{\begin{matrix}
    4 -\lambda & 5 \\
    5 & 12 -\lambda
    \end{matrix}} = 0$$
    $$\implies \lambda^2 - 16\lambda +23 = 0 \implies \lambda =
    8 \pm \sqrt{41}$$
    $8 > \sqrt{41}$, so we have that both eigenvalues are strictly 
    positive. This means that $\nabla^2 f$ is positive definite, and
    that $f$ is a convex function. The plot is below and the 
    code for the plot is in \il{H1P3.py}:
    \begin{figure}[h!]
  	\includegraphics[width=\linewidth]{HW1P3surface.jpg}
  	\centering
	\end{figure}
	\pagebreak
    \end{proof}
\item
    \be[(i)] 
    \item Show that the sequence $x_k = 1 + \parens{0.5}^{2^k}$ 
    is Q-quadratically convergent to 1.
    \begin{proof}
    To show this, we'll use the definition for Q-quadratic 
    convergence. 
    \begin{align*}
    \frac{\abs{x_{k+1} - x^*}}{\abs{x_k - x^*}^2} &=
    \frac{\abs{1 + \parens{0.5}^{2^{k+1}}-1}}{\abs{1 + 
    \parens{0.5}^{2^k}-1}^2} \\
    &= \frac{\abs{\parens{0.5}^{2^{k+1}}}}{\abs{
    \parens{0.5}^{2^k}}^2}
    \end{align*}
    Both $\parens{0.5}^{2^k}$ and $\parens{0.5}^{2^{k+1}}$ are 
    strictly positive, so
    \begin{align*}
    \frac{\abs{x_{k+1} - x^*}}{\abs{x_k - x^*}^2} &=
    \frac{\abs{\parens{0.5}^{2^{k+1}}}}{\abs{
    \parens{0.5}^{2^k}}^2} \\
    &= \frac{\parens{0.5}^{2^{k+1}}}{\parens{
    \parens{0.5}^{2^k}}^2} \\
    &= \frac{\parens{0.5}^{2^{k+1}}}{\parens{0.5}^{2^k\cdot 2}} \\
    &= \frac{\parens{0.5}^{2^{k+1}}}{\parens{0.5}^{2^{k+1}}} \\
    &= 1
    \end{align*}
    As the quotient equals 1 for all $k$, $x_k$ converges 
    Q-quadratically to 1.
    \end{proof}
    
    \item Does the sequence $x_k = 1/k!$ converge Q-superlinearly?
    Q-quadratically? 
    \begin{proof}[Answer]
    First we'll find the limit of $x_k$ which is clearly:
    $$\lim_{k\to\infty} \frac{1}{k!} = 0$$
    This sequence converges Q-superlinearly, to demonstrate:
    \begin{align*}
    \lim_{k\to\infty} \frac{\abs{x_{k+1}-x^*}}{\abs{x_k - x^*}} 
    &= \lim_{k\to\infty} \frac{\abs{\frac{1}{(k+1)!}}}
    {\abs{\frac{1}{k!}}} \\
    &= \lim_{k\to\infty} \frac{\frac{1}{(k+1)!}}
    {\frac{1}{k!}} \\
    &= \lim_{k\to\infty} \frac{k!}{(k+1)!} \\
    &= \lim_{k\to\infty} \frac{k!}{(k+1)\cdot k!} \\
    &= \lim_{k\to\infty}\frac{1}{k+1} \\
    &= 0
    \end{align*}
    As desired.     
    \end{proof}
    \ee
\ee
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
	
\end{document}
