\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{color}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{mdframed}
\usepackage{multirow}
\usetikzlibrary{positioning}
\tikzset{main node/.style={circle,fill=gray!20,draw,minimum size=.5cm,inner sep=0pt},}

% In line code stuff%
\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codewhite}{rgb}{1,1,1}
\definecolor{codegray}{rgb}{0.85,0.85,0.85}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblack}{rgb}{0,0,0}
\definecolor{codeorange}{rgb}{0.8,0.4,0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{codegray},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codegreen},
    numberstyle=\color{codegray},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily ,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}
\setlength{\hoffset}{-1in}
\addtolength{\textwidth}{1.5in}
\setlength{\voffset}{-1in}
\addtolength{\textheight}{1.5in}

% Custom commands%
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\BigO}[1]{\ensuremath\mathcal{O}\left(#1\right)}
\newcommand{\il}[1]{\lstinline!#1!}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\parens}[1]{\left(#1\right)}
\newcommand{\bracks}[1]{\left\{#1\right\}}
\newcommand{\sqbracks}[1]{\left[#1\right]}
\newcommand{\vep}{\varepsilon}
\newcommand{\ceiling}[1]{\left\lceil#1\right\rceil}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\distrib}[2]{\text{#1}\left(#2\right)}
\newcommand{\dd}[1]{\frac{d}{d#1}}
\newcommand{\abracks}[1]{\left< #1\right>}

\newenvironment{answer}
    {\begin{mdframed}[
    backgroundcolor=lightgray,
    outerlinewidth=0
    ]\emph{Answer.} }
    {\end{mdframed}}

\newenvironment{centeredtable}[1]
    {\begin{center}
    \begin{tabular}{#1}}
    {\end{tabular} 
    \end{center}
    }

\begin{document}
	\begin{center}
		\textbf{Fall 2020, Math 693A:\ Homework 3} \\
		\textbf{Due:\ Friday, October 16th, 2020} \\
		\textbf{Joseph Diaz: 819947915}
	\end{center}
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
Write a program that implements the dogleg method. Choose $B_k$ to be 
the exact Hessian. Apply it to solve Rosenbrock's function.
\be[1.]
    \item Use an initial trust radius of 1. Set a maximum trust 
    region radius of 300. Use the initial point $\bm{x}_0 = 
    \sqbracks{-1.2, 1}$ and then try another point $\bm{x}_0 = 
    \sqbracks{2.8, 4}$. Do the following for each initial point:
    \be[a.]
        \item Use $\norm{\nabla f\parens{\bm{x}_k}} < 10^{-8}$ as the
        stopping criteria.
        
        \item State the total number of iterations obtained in your
        optimization algorithm.
        \begin{answer}
        \begin{center}
        \begin{tabular}{|c|c|c|}
        \hline
        $\bm{x}_0$ & $(-1.2, 1)$ & $(2.8, 4)$ \\
        \hline
        $k$ & 24 & 19 \\
        \hline
        \end{tabular}    
        \end{center}                 
        \end{answer}
        

        \item Plot the objective funtion $f(\bm{x})$. On the same 
        figure, plot the $\bm{x}_k$ values at the different iterates
        of your optimization algorithm.
        \begin{figure}[h!]
  	    \includegraphics[width=\linewidth]{HW3;HardStart.jpg}
  	    \centering
	    \end{figure}
        \begin{figure}[h!]
  	    \includegraphics[width=\linewidth]{HW3;EasyStart.jpg}
  	    \centering
	    \end{figure}
	    \pagebreak
        \item Plot the size of the objective function as a function 
        of the iteration number. Use a semi-log plot.
        \begin{figure}[h!]
  	    \includegraphics[width=.85\linewidth]{HW3;Objective0.jpg}
  	    \centering
	    \end{figure}
        \begin{figure}[h!]
  	    \includegraphics[width=.85\linewidth]{HW3;Objective1.jpg}
  	    \centering
	    \end{figure}
        
        \item You should hand in your code and the first 4 and last 4 
        values of $\bm{x}_k$ obtained from your program.
        \begin{answer}
        The code is attached to this submission.\\    
        The first 4 and last 4 for each starting point are:
        \begin{centeredtable}{|c|c|c|}
            \hline
            $\bm{x}_0$ & $\parens{-1.2, 1}$ & $\parens{2.8, 4}$ \\
            \hline
            \parbox[t]{3mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}
            {Next 4}}} & $\parens{-1.1752809,\ 1.38067416}$ & 
            $\parens{2.37618752,\ 4.90574996}$ \\\cline{2-3}
            & $\parens{-1.07407754,\ 1.15207435}$ & 
            $\parens{2.36695777,\ 5.60240391}$ \\\cline{2-3}
            & $\parens{-0.96464932,\ 0.92729565}$ & 
            $\parens{2.26306222,\ 5.11331732}$ \\\cline{2-3}
            & $\parens{-0.73010519,\ 0.48572001}$ & 
            $\parens{2.15304408,\ 4.62557146}$ \\\cline{2-3}
            \hline
            \parbox[t]{3mm}{\multirow{4}{*}{\rotatebox[origin=c]{90}
            {Last 4}}} & $\parens{0.99109917,\ 0.98164169}$ & 
            $\parens{1.02252306,\ 1.04536819}$ \\\cline{2-3}
            & $\parens{0.99899574,\ 0.99793014}$ & 
            $\parens{1.00080454,\ 1.00113802}$ \\\cline{2-3}
            & $\parens{0.99998763,\ 0.99997428}$ & 
            $\parens{1.00006936,\ 1.00013818}$ \\\cline{2-3}
            & $\parens{1.0,\ 0.99999999}$ & 
            $\parens{1.00000001,\ 1.00000001}$ \\\cline{2-3}
            \hline
        \end{centeredtable}
        \end{answer}

        \item Determine the minimizer of the Rosenbrock function 
        $\bm{x}^*$.
        \begin{answer}
        The minimizer of the Rosenbrock function is $\bm{x} = 
        \parens{1,\ 1}$    
        \end{answer}
    \ee
    \pagebreak
    \item Experiment with the update rule for the trust region by 
    changing the constants in Algorithm 4.1, or by designing your 
    own rules.
    \begin{answer}
        My first bit of experimentation was with the maximum trust radius 
        and how it would affect the number of iterations necessary to find
        the minimum. I was under the impression that starting points 
        \emph{further} from the minimum would benefit disproportionately
        more from a larger maximum, because that would allow for larger 
        stepsizes when taking the unconstrained minimizer of the quadratic
        model path during the dogleg step. The table below has the number 
        of iterations for different combinations of starting point and 
        maximum trust radius; and my intuition in this case was \emph{dead
        wrong}. For ``distant'' starting points, the change in iterations 
        is slight, which I didn't expect, and it seems that smaller trust 
        radii are better than large for these points; though the results
        are a little inconsistent.
    \begin{center}
        \begin{tabular}{|c||c|c|c|c|c|c|}
        \hline
         & \multicolumn{6}{|c|}{Max Trust Radius $\widehat{\Delta}$}
         \\
        \hline 
        $\bm{x}_0$ & 50 & 100 & 150 & 200 & 250 & 300\\
        \hline
        \hline
        $(-1.2, 1)$ & 24 & 24 & 24 & 24 & 24 & 24 \\
         \hline
        $(2.8, 4)$ & 19 & 19 & 19 & 19 & 19 & 19 \\
        \hline
        $(1.2, 1.2)$ & 8 & 8 & 8 & 8 & 8 & 8 \\
        \hline
        $(64, 89)$ & 71 & 71 & 71 & 71 & 71 & 71 \\
        \hline
        $(-90, -100)$ & 65 & 64 & 64 & 65 & 65 & 65 \\
        \hline
        $(200, -250)$ & 64 & 82 & 82 & 72 & 82 & 82 \\
        \hline
        \end{tabular}        
        \end{center}
        In addition to that, I decided to compute the rate of convergence
        for my dogleg method from each starting point; the computation 
        uses linear regression on the $q$-convergence formula.
        It seems that my method never gets to quadratic convergence, but 
        remains super-linear even for ``distant'' starting points.
        \begin{lstlisting}[language=Python]
Rate of convergence; x0 = [-1.2  1. ]: 1.7096584548064946
Rate of convergence; x0 = [2.8 4. ]: 1.667303158458797
Rate of convergence; x0 = [1.2 1.2]: 1.6819089199401087
Rate of convergence; x0 = [64. 89.]: 1.3357445881203478
Rate of convergence; x0 = [ -90. -100.]: 1.2881806845938344
Rate of convergence; x0 = [ 200. -250.]: 1.2304148974286027\end{lstlisting}
        Further, I found that choosing a random value for $\eta \in [0, 
        .25]$ was more reliable than choosing an arbitrary one for each
        optimization. The total number of iterations doesn't seem to 
        change much for this choice. The same cannot be said for the 
        value that you compare against $\rho_k$; changing this value has a
        more noticeable effect on the overall iterations and time to optimize.
    \end{answer}
\ee
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
	
\end{document}
