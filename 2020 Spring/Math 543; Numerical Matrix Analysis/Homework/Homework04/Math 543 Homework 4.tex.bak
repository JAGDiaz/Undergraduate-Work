\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{color}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.5,0}
\definecolor{codewhite}{rgb}{1,1,1}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{codeblack}{rgb}{0,0,0}
\definecolor{codeorange}{rgb}{0.8,0.4,0}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{codewhite},   
    commentstyle=\color{codegray},
    keywordstyle=\color{codegreen},
    numberstyle=\color{codegray},
    stringstyle=\color{codeorange},
    basicstyle=\ttfamily ,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}


\setlength{\hoffset}{-1in}
\addtolength{\textwidth}{1.5in}
\setlength{\voffset}{-1in}
\addtolength{\textheight}{1.5in}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\BigO}[1]{\ensuremath\mathcal{O}\left(#1\right)}
\newcommand{\il}[1]{\lstinline!#1!}

\begin{document}
	\begin{center}
		\textbf{Spring 2020, Math 543:\ Homework 4} \\
		\textbf{Due:\ Friday, March 6th, 2020} \\
		\textbf{Joseph Diaz: 819947915}
	\end{center}
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
\be[1.]
\item  Implement modified Gram-Schmidt QR-factorization.\\
Work through experiment \#1 and \#2 in ``Lecture 9'' of Trefethen
\& Bau. Make sure your versions of classical and modified GS can
reproduce figure 9.1.\\
Note that depending on your coding environment, you may have
to use larger (and worse conditioned) matrices to achieve the loss
of orthogonality in classical Gram-Schmidt.
	\begin{figure}[h]
  	\includegraphics[width=\linewidth]{"GS Stability; Modified vs Classic".jpg}
  	\centering
	\end{figure}
	\pagebreak
\item Do exercises 9.1(a,b), and 9.2(a,b).
	\be
		\item[9.1]
		\be[(a)]
			\item Run the code from Experiment 1 to produce a plot of approximate Legendre Polynomials.
	\begin{figure}[h]
  	\includegraphics[width=\linewidth]{"Legendre Polynomials Approximations".jpg}
  	\centering
	\end{figure}
		\pagebreak

			\item For $k \in \{0,1,2,3\}$, plot the difference on the 257-point grid between these approximations and the exact polynomials (7.11). How big are the errors, and how are they distributed?
	\begin{figure}[h]
  	\includegraphics[width=\linewidth]{"Legendre Polynomials Approximations vs True".jpg}
  	\centering
	\end{figure}\\
	The difference between both the ``true'' and approximate Legendre Polynomials is really only pronounced for $P_2(x)$ and $P_3(x)$. Where the approximation for $P_2(x)$ is only significantly different at $x =0$, but the approximation improves as $x$ gets closer to the boundaries. Conversely, $P_3(x)$ is well approximated at $x \in \{-1, 0, 1\}$, while less accurate near $x \in \{-0.6, 0.6\}$. The greatest error between the approximations and their true values on $[-1,1]$ is only about 0.0155.
		\ee
		\item[9.2]
		In Experiment 2, the singular values of $A$ match the diagonals elements of a $QR$ factor $R$ approximately. Consider now a very different example. Suppose $Q = I$ and $A=R$, the $m\times m$ matrix (a \textit{Toeplitz matrix}) with 1's on the main diagonal, 2's on the superdiagonal, and 0's everywhere else.
		\be[(a)]
			\item What are the eigenvalues, determinant, and rank of $A$?
			\begin{proof}[Solution]
			We have that 
			$$A = R = \left(
			\begin{matrix}
			1 & 2 & 0 & \cdots & 0\\
			0 & 1 & 2 & \cdots & 0\\
			\vdots & 0 &  \ddots & \ddots & 0\\
			\vdots & \ddots & \ddots & 1 & 2\\
			0 & \cdots & \cdots & 0 & 1
			\end{matrix}\right)_{m\times m}
			$$
			So the eigenvalues of $A$ can be found by solving $\left\vert A-\lambda I_m\right\vert = 0$:
			$$\left\vert A-\lambda I_m\right\vert = 0 \implies \left\vert
			\begin{matrix}
			1-\lambda & 2 & 0 & \cdots & 0\\
			0 & 1-\lambda & 2 & \cdots & 0\\
			\vdots & 0 &  \ddots & \ddots & 0\\
			\vdots & \ddots & \ddots & 1-\lambda & 2\\
			0 & \cdots & \cdots & 0 & 1-\lambda
			\end{matrix}\right\vert = 0 \implies (1-\lambda)^m= 0$$
			So, the eigenvalues of $A$ are $\lambda = 1$ with multiplicity $m$.\\
			As $A$ is upper-triangular, we know that it's determinant is the product of the elements on the main diagonal. So:
			$$\left\vert A\right\vert = 1$$
			And as $A$ has $m$ linearly independent columns:
			$$\text{Rank}(A) = m$$ 
			\end{proof}
			\item What is $A^{-1}$?
			$$A = \left(
			\begin{matrix}
			1 & 2 & 0 & \cdots & 0\\
			0 & 1 & 2 & \cdots & 0\\
			\vdots & 0 &  \ddots & \ddots & 0\\
			\vdots & \ddots & \ddots & 1 & 2\\
			0 & \cdots & \cdots & 0 & 1
			\end{matrix}\right) \implies A^{-1} = \left(
			\begin{matrix}
			1 &-2 & 4 & \cdots & (-2)^{m-1}\\
			0 & 1 &-2 & \ddots & \vdots\\
			\vdots & 0 &  \ddots & \ddots & 4\\
			\vdots & \ddots & \ddots & 1 &-2\\
			0 & \cdots & \cdots & 0 & 1
			\end{matrix}\right)$$
		\ee
		
		
	\ee
\ee
\noindent\makebox[\linewidth]{\rule{\paperwidth}{0.4pt}}
\subsection*{Source code:}
\begin{lstlisting}[language=python]

\end{lstlisting}
\end{document}
